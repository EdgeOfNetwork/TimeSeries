{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_normal = pd.read_csv('2.2kW_cur_L-DSF-01_training_with_label.csv')\n",
    "vib_normal = pd.read_csv('2.2kW_vib_L-DSF-01_training_with_label.csv')\n",
    "cur_validation = pd.read_csv('2.2kW_cur_L-DSF-01_validation_with_label.csv')\n",
    "vib_validation = pd.read_csv('2.2kW_vib_L-DSF-01_validation_with_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36768\n",
      "24487\n",
      "4599\n",
      "3064\n"
     ]
    }
   ],
   "source": [
    "print(len(cur_normal))\n",
    "print(len(vib_normal))\n",
    "print(len(cur_validation))\n",
    "print(len(vib_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_cur_normal = cur_normal.to_numpy()\n",
    "np_vib_normal = vib_normal.to_numpy()\n",
    "np_cur_validation = cur_validation.to_numpy()\n",
    "np_vib_validation = vib_validation.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 3 3]\n",
      "[0 0 0 ... 3 3 3]\n",
      "[0 0 0 ... 3 3 3]\n",
      "[0 0 0 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "normal_cur_labels = np_cur_normal[:,-1]\n",
    "normal_vib_labels = np_vib_normal[:,-1]\n",
    "val_cur_labels = np_cur_validation[:, -1]\n",
    "val_vib_labels = np_vib_validation[:, -1]\n",
    "\n",
    "print(normal_cur_labels)\n",
    "print(normal_vib_labels)\n",
    "print(val_cur_labels)\n",
    "print(val_vib_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_normal_dropped = cur_normal.drop(columns = ['Date','WATT','LABEL'])\n",
    "vib_normal_dropped = vib_normal.drop(columns = ['Date','WATT','LABEL'])\n",
    "cur_validation_dropped = cur_validation.drop(columns = ['Date','WATT','LABEL'])\n",
    "vib_validation_dropped = vib_validation.drop(columns = ['Date','WATT','LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_AbsMax</th>\n",
       "      <th>S_AbsMax</th>\n",
       "      <th>T_AbsMax</th>\n",
       "      <th>R_AbsMean</th>\n",
       "      <th>S_AbsMean</th>\n",
       "      <th>T_AbsMean</th>\n",
       "      <th>R_P2P</th>\n",
       "      <th>S_P2P</th>\n",
       "      <th>T_P2P</th>\n",
       "      <th>R_RMS</th>\n",
       "      <th>...</th>\n",
       "      <th>T_1x</th>\n",
       "      <th>R_2x</th>\n",
       "      <th>S_2x</th>\n",
       "      <th>T_2x</th>\n",
       "      <th>R_3x</th>\n",
       "      <th>S_3x</th>\n",
       "      <th>T_3x</th>\n",
       "      <th>R_4x</th>\n",
       "      <th>S_4x</th>\n",
       "      <th>T_4x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.403320</td>\n",
       "      <td>3.123047</td>\n",
       "      <td>4.043945</td>\n",
       "      <td>2.057587</td>\n",
       "      <td>1.889543</td>\n",
       "      <td>2.245951</td>\n",
       "      <td>6.726562</td>\n",
       "      <td>6.166016</td>\n",
       "      <td>7.607422</td>\n",
       "      <td>2.296682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014614</td>\n",
       "      <td>3.282771</td>\n",
       "      <td>3.058104</td>\n",
       "      <td>3.579351</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.019693</td>\n",
       "      <td>0.023455</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>0.024256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.283203</td>\n",
       "      <td>3.083008</td>\n",
       "      <td>4.003906</td>\n",
       "      <td>2.059709</td>\n",
       "      <td>1.883498</td>\n",
       "      <td>2.254960</td>\n",
       "      <td>6.526367</td>\n",
       "      <td>6.005859</td>\n",
       "      <td>7.487305</td>\n",
       "      <td>2.287841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>3.433224</td>\n",
       "      <td>3.183656</td>\n",
       "      <td>3.761500</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.014099</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.039196</td>\n",
       "      <td>0.035626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.283203</td>\n",
       "      <td>3.083008</td>\n",
       "      <td>3.963867</td>\n",
       "      <td>2.059209</td>\n",
       "      <td>1.884539</td>\n",
       "      <td>2.255280</td>\n",
       "      <td>6.526367</td>\n",
       "      <td>6.085938</td>\n",
       "      <td>7.407227</td>\n",
       "      <td>2.288444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>3.432319</td>\n",
       "      <td>3.184894</td>\n",
       "      <td>3.755824</td>\n",
       "      <td>0.022333</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.020837</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.026358</td>\n",
       "      <td>0.023356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   R_AbsMax  S_AbsMax  T_AbsMax  R_AbsMean  S_AbsMean  T_AbsMean     R_P2P  \\\n",
       "0  3.403320  3.123047  4.043945   2.057587   1.889543   2.245951  6.726562   \n",
       "1  3.283203  3.083008  4.003906   2.059709   1.883498   2.254960  6.526367   \n",
       "2  3.283203  3.083008  3.963867   2.059209   1.884539   2.255280  6.526367   \n",
       "\n",
       "      S_P2P     T_P2P     R_RMS  ...      T_1x      R_2x      S_2x      T_2x  \\\n",
       "0  6.166016  7.607422  2.296682  ...  0.014614  3.282771  3.058104  3.579351   \n",
       "1  6.005859  7.487305  2.287841  ...  0.015374  3.433224  3.183656  3.761500   \n",
       "2  6.085938  7.407227  2.288444  ...  0.019783  3.432319  3.184894  3.755824   \n",
       "\n",
       "       R_3x      S_3x      T_3x      R_4x      S_4x      T_4x  \n",
       "0  0.022117  0.019693  0.023455  0.031669  0.027953  0.024256  \n",
       "1  0.013864  0.011091  0.014099  0.030992  0.039196  0.035626  \n",
       "2  0.022333  0.019714  0.020837  0.024400  0.026358  0.023356  \n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_normal_dropped.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_normal_dropped_np = cur_normal_dropped.to_numpy()\n",
    "vib_normal_dropped_np = vib_normal_dropped.to_numpy()\n",
    "cur_validation_dropped_np = cur_validation_dropped.to_numpy()\n",
    "vib_validation_dropped_np = vib_validation_dropped.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Train에서 Test set 분리 : 1. CURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cur_normal_dropped_np, normal_cur_labels, stratify=normal_cur_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.24316406e+00  3.16308594e+00  3.96386719e+00  2.01802881e+00\n",
      "  1.86465918e+00  2.20753369e+00  6.48632812e+00  6.20605469e+00\n",
      "  7.36718750e+00  2.24058769e+00  2.08956767e+00  2.46386464e+00\n",
      "  6.39920396e-03 -7.89028211e-04 -7.12709533e-03  1.47027723e+00\n",
      "  1.50019839e+00  1.50548275e+00  2.89492268e+00  2.97001853e+00\n",
      "  2.99009426e+00  1.11028528e+00  1.12061641e+00  1.11611644e+00\n",
      "  1.60709503e+00  1.69633463e+00  1.79560892e+00 -3.14159265e+00\n",
      " -3.14159265e+00  3.14159265e+00  1.60696777e-01  2.10028418e+00\n",
      " -2.26098096e+00  1.13101505e-02  1.01238237e-02  1.31138674e-02\n",
      "  3.21958821e+00  3.00243212e+00  3.52495620e+00  1.22113268e-02\n",
      "  1.18758799e-02  1.16075196e-02  3.26346269e-02  3.53758430e-02\n",
      "  3.65411011e-02]\n",
      "[ 3.36328125e+00  3.20312500e+00  4.16406250e+00  2.13224023e+00\n",
      "  1.93662939e+00  2.34612891e+00  6.72656250e+00  6.28613281e+00\n",
      "  7.84765625e+00  2.39082191e+00  2.18733054e+00  2.64398306e+00\n",
      " -5.31305087e-03 -6.09845670e-03  3.81981107e-03  1.48597871e+00\n",
      "  1.51111169e+00  1.53145200e+00  2.81349375e+00  2.87388335e+00\n",
      "  2.96811896e+00  1.12127230e+00  1.12945231e+00  1.12695558e+00\n",
      "  1.57734630e+00  1.65396901e+00  1.77486518e+00  3.14159265e+00\n",
      "  3.14159265e+00 -3.14159265e+00  2.42136230e-01  2.13954248e+00\n",
      " -2.38167871e+00  1.70233751e-02  1.46587946e-02  2.60306805e-02\n",
      "  3.62261805e+00  3.31144778e+00  3.98925014e+00  2.09130453e-02\n",
      "  2.26975179e-02  2.38534731e-02  2.96530144e-02  3.10964354e-02\n",
      "  2.72945369e-02]\n",
      "0\n",
      "3\n",
      "(29414, 45)\n",
      "(7354, 45)\n",
      "(29414,)\n",
      "(7354,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_test[0])\n",
    "print(y_train[0])\n",
    "print(y_test[0])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 변형 객체 생성\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "std_scaler.fit(x_train)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "X_train_scaled = std_scaler.transform(x_train)\n",
    "\n",
    "# 테스트 데이터의 스케일링\n",
    "X_test_scaled = std_scaler.transform(x_test)\n",
    "\n",
    "# 스케일링 된 결과 값으로 본래 값을 구할 수도 있다.\n",
    "# X_origin = std_scaler.inverse_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45620424,  0.02795789, -0.31167784, -0.39923818, -0.18190779,\n",
       "       -0.49906522, -0.36454482, -0.0193779 , -0.38960471, -0.47813089,\n",
       "       -0.25956036, -0.56645338,  0.02414326,  0.04492438, -0.04812781,\n",
       "       -0.03068375, -0.02749687, -0.02673015, -0.00239409,  0.18532114,\n",
       "        0.07353428, -0.04976575, -0.10686775, -0.05710098, -0.02018284,\n",
       "       -0.00960171, -0.0031203 , -0.25181912, -1.22993029,  0.98354463,\n",
       "       -0.77563049, -0.03417307,  0.2758988 , -0.24321982, -0.14717149,\n",
       "        0.08485812, -0.20841773, -0.10348383, -0.23451495, -0.57820507,\n",
       "       -0.49383409, -0.62062374, -0.19067947, -0.09230915, -0.06799548])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation도 Scale을 맞춰줘야겠지??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# # load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# # truncate and pad input sequences\n",
    "# max_review_length = 500\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# # create the model\n",
    "\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
